{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "torch.manual_seed(1)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt; plt.style.use(\"fivethirtyeight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I will explore usage of RNNs in pytorch.\n",
    "https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 3\n",
    "hidden_size = 4\n",
    "lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size)  # LSTM(input_size, output_size)\n",
    "# make a sequence of length 5\n",
    "input_sequence = [torch.randn(1, 1, input_size) for _ in range(5)]\n",
    "\n",
    "# initalize the hidden state\n",
    "hidden = (torch.randn(1, 1, hidden_size),\n",
    "          torch.randn(1, 1, hidden_size))\n",
    "for i in input_sequence:\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[ 0.0939,  1.2381, -1.3459]]]),\n",
       " tensor([[[ 0.5119, -0.6933, -0.1668]]]),\n",
       " tensor([[[-0.9999, -1.6476,  0.8098]]]),\n",
       " tensor([[[ 0.0554,  1.1340, -0.5326]]]),\n",
       " tensor([[[ 0.6592, -1.5964, -0.3769]]])]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "(All the information about LSTM can be found here: https://pytorch.org/docs/stable/nn.html#lstm)\n",
    "\n",
    "A little bit of explanation of what was just done.\n",
    "\n",
    "First of all, for explanation of a difference between `reshape` and `view` see here: https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view. Basically view works only in the special case when reshape works\n",
    "\n",
    "Input data dimensionality:\n",
    "* first dimension: sequence itself\n",
    "* second dimension: samples from a mini-batch\n",
    "* third dimension: elements of a vector input\n",
    "\n",
    "Why is `hidden` a tuple of two vectors? Don't we need just a single one? It is becasue while in simple RNN the model contained just a hidden vectors, in LSTM we also have cell memory (often denoted as $c$). Because of that, we have to initialize that as well.\n",
    "\n",
    "<img src=\"img/lstm_equations.png\">\n",
    "\n",
    "In the for loop we basically iterated over all elements of a sequence and performed the forward pass. We can also do it without a for loop, treating input as one tensor (which actually makes more sense)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs shape: torch.Size([5, 1, 3])\n",
      "tensor([[[-0.2566,  0.1294,  0.0441, -0.5235]],\n",
      "\n",
      "        [[-0.4444,  0.0762,  0.0304, -0.3889]],\n",
      "\n",
      "        [[-0.1741,  0.1061, -0.0179, -0.1505]],\n",
      "\n",
      "        [[-0.1409,  0.1110,  0.0773, -0.2373]],\n",
      "\n",
      "        [[-0.2308,  0.1164, -0.0115, -0.2423]]], grad_fn=<StackBackward>)\n",
      "(tensor([[[-0.2308,  0.1164, -0.0115, -0.2423]]], grad_fn=<StackBackward>), tensor([[[-0.4093,  0.6065, -0.0288, -0.4107]]], grad_fn=<StackBackward>))\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.cat(input_sequence).view(len(inputs), 1, -1)\n",
    "print(\"Inputs shape: {}\".format(inputs.shape))\n",
    "hidden = (torch.randn(1, 1, hidden_size), torch.randn(1, 1, hidden_size))\n",
    "out, hidden = lstm(inputs, hidden)\n",
    "print(out)\n",
    "print(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of LSTM is a tuple of\n",
    "* all the hidden states (for each input sequence) \n",
    "* last hidden state\n",
    "\n",
    "It is kind of redundant, becasuse we could have taken last hidden state from the first element of the tuple as well (check that the last element of first element of a tuple is the same as first element of second element of the tuple), but it's not the case for $c$, which we have to input to LSTM later on in a forward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "And that's it. As we see most of the details of LSTM were abstracted away for us, we just need a basic understanding how LSTM works. \n",
    "\n",
    "Note however, that we ignored the our approach was to perform a forward pass on a single observation (sequence). That's perfectly fine and we can optimize (do backpropagation) one sample at a time, but what if we want to process more elements at once? Well, we can create mini batches. But we have to keep in mind that the sequences can be of different lengths (that's the whole point of sequence models like RNNs) so it's not clear so far how to deal with that. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
